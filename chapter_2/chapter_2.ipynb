{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. tf.keras简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras简介\n",
    "- Keras是什么？\n",
    "    - 基于python的高级神经网络API\n",
    "    - Francois Chollet于2014-2015年编写Kearas\n",
    "    - 以Tensorflow/CNTK或者Theano为后端进行运行， Kearas必须有后端才可以运行；后端可以进行切换，现在多用Tensorflow\n",
    "    - 极方便快速实验，帮助用户减少时间验证自己的想法\n",
    "\n",
    "- Tensorflow-Kears是什么？\n",
    "    - Tensorflow对Keras API规范的实现\n",
    "    - 相对于Tensorflow后端的Keras，Tensorflow-Keras与Tensorflow结合的更加紧密\n",
    "    - 实现在tf.keras空间下\n",
    "\n",
    "- tf.keras与keras 区别和联系\n",
    "    - 联系：基于同一套API；keras可以轻松转到tf.keras，反之则不行；\n",
    "    - 区别：tf.keras全面支持eager mode; tf.keras支持基于 tf.data的模型训练\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 机器学习基础\n",
    "- 分类问题和回归问题\n",
    "    - 分类问题预测的是类别，模型的输出是概率分布；\n",
    "    - 回归问题预测的是值，模型输出是是数值；\n",
    "\n",
    "- 目标函数\n",
    "    - 为什么需要目标函数？ 参数是逐步调整的，并不能直接计算得到；目标函数可以帮助衡量模型的好坏；\n",
    "    - 分类问题\n",
    "        - 需要衡量目标类别与当前预测差距\n",
    "            - 三分类case [0.2, 0.7, 0.1]\n",
    "            - 三分类真是类别 2 -> one_hoe -> [0, 0, 1]\n",
    "        - One-hot编码，把正整数变为向量表达\n",
    "        - 损失函数\n",
    "            - 平方差损失 \n",
    "            - 交叉熵损失\n",
    "    - 回归问题\n",
    "        - 预测值与真实值的差距\n",
    "        - 平方差损失\n",
    "        - 绝对值损失\n",
    "\n",
    "    - 模型的训练就是调整参数，使得目标函数逐步变小的过程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络\n",
    "\n",
    "- 神经网络争相计算\n",
    "- 神经网络训练  SGD 梯度下降法 \n",
    "- 深度神经网络：层次非常深的神经网络\n",
    "- 激活函数\n",
    "    ![image1.png](./image/激活函数.jpg)\n",
    "    - *为什么要使用激活函数 ？*\n",
    "    \n",
    "- 归一化与批归一化\n",
    "    - 归一化\n",
    "        - Min-Max归一化 x = (x-min)/(max-min)\n",
    "        - Z-score归一化 x = (x-u)/std\n",
    "    - 批归一化\n",
    "        每层的激活值都进行归一化\n",
    "    - 为什么要归一化(加速收敛)\n",
    "     ![image2.png](./image/归一化.jpeg)\n",
    "    \n",
    "- dropout  随机的丢弃神经元\n",
    "    - dropout 可以防止过拟合  \n",
    "     \n",
    "     ![image3.png](./image/dropout.jpeg)\n",
    "    \n",
    "    - *dropout为什么有效？*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wide&Deep 模型\n",
    "\n",
    "- 稀疏特征\n",
    "    - 离散值特征\n",
    "    - One-hot表示  专业={计算机，人文，其他}=[0,1,0]\n",
    "    - 叉乘（特征交叉） 稀疏特征做叉乘获取共现信息；实现记忆效果；\n",
    "    - 优缺点： \n",
    "        - 优点：有效广泛用于工业界\n",
    "        - 缺点：需要人工设计\n",
    "- 密集特征\n",
    "    - 向量表达 \n",
    "    - word2vec工具\n",
    "    - 优缺点 \n",
    "        - 优点： 带有语义信息，不同向量之间有相关性；兼容没有出现过的特征组合；更少人工参与\n",
    "        - 缺点： 过度泛化，推荐不相关的产品\n",
    "- wide&deep模型结构\n",
    "    ![wide_n_deep.png](./image/wide_n_deep.jpg)\n",
    "    稀疏特征 -> 密集特征 -> 隐藏层 -> 输出层\n",
    "    \n",
    "    ![wide_n_deep_algo.png](./image/wide_n_deep_algo.jpg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超参数搜索\n",
    "\n",
    "- 为什么要超参数搜索\n",
    "    - 神经网络有很多训练过程中不变的参数\n",
    "        - 网络结构参数： 几层，每层的宽度，每层激活函数等\n",
    "        - 训练参数：batch_size，learning_rate，学习率衰减算法等\n",
    "    - 手工试耗费人力\n",
    "- 搜索策略\n",
    "    - 网格搜索\n",
    "        ![grid_search.png](./image/grid_search.jpeg)\n",
    "    - 随机搜索\n",
    "        ![random_search.png](./image/random_search.jpeg)\n",
    "    - 遗传算法搜索\n",
    "        ![genetic_search.png](./image/genetic_search.png)\n",
    "    - 启发式搜索\n",
    "        ![heuristic_search.png](./image/heuristic_search.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
